# Customer Churn Prediction & Analysis

<img src="https://img.shields.io/badge/Python-3.9%2B-blue?style=for-the-badge&logo=python&logoColor=white"> <img src="https://img.shields.io/badge/scikit--learn-1.4+-orange?style=for-the-badge"> <img src="https://img.shields.io/badge/ğŸ“Š-Exploratory%20Data%20Analysis-important?style=for-the-badge"> <img src="https://img.shields.io/badge/ğŸ”„-Imbalanced%20Learning-ff69b4?style=for-the-badge">

**End-to-end machine learning project** focused on understanding and predicting customer churn in a **telecom / banking / SaaS / subscription** business.

## ğŸ¯ Business Problem

> "We are losing too many customers every month and we don't know exactly why nor who is going to leave next."

**Goal**:  
Build a model that can **predict which customers are likely to churn** in the next 30 days with good enough precision/recall trade-off â†’ enable **targeted retention campaigns**.

## ğŸ“Š Dataset

Common public datasets used in this repository (choose one):

| Dataset                          | Rows   | Churn Rate | Most used for          | Link / Source                                 |
|-------------------------------|--------|------------|------------------------|-----------------------------------------------|
| Telco Customer Churn (IBM)    | ~7k    | ~27%       | beginners & comparison | Kaggle                                        |
| WA_Fn-UseC_-Telco-Customer-Churn | 7043 | 26.5%      | classic benchmark      | https://www.kaggle.com/datasets/blastchar/telco-customer-churn |
| Bank Customer Churn           | 10k    | ~20%       | banking use-case       | Kaggle / Ravel                            |
| E-commerce Churn              | ~5â€“20k | 15â€“30%     | online retail          | various Kaggle datasets                       |
| Synthetic / own generated     | â€”      | â€”          | advanced experiments   | â€”                                             |

## Project Structure (2025 recommended layout)

```text
customer-churn-analysis/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  â† never modify!
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ external/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-eda.ipynb
â”‚   â”œâ”€â”€ 02-feature-engineering.ipynb
â”‚   â”œâ”€â”€ 03-model-baseline.ipynb
â”‚   â”œâ”€â”€ 04-model-tuning.ipynb
â”‚   â””â”€â”€ 05-interpretability.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ make_dataset.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train_model.py
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â””â”€â”€ visualization/
â”‚       â””â”€â”€ custom_plots.py
â”œâ”€â”€ models/                   â† saved models (.joblib / .pkl / .cbm / .onnx)
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ churn_report.md / churn_dashboard.html
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml           (optional â€“ conda)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                â† you're reading this
â””â”€â”€ churn_predictor/          (optional package structure)
    â”œâ”€â”€ __init__.py
    â””â”€â”€ pipeline.py

## ğŸ† Model Performance Comparison  
(5-fold stratified cross-validation â€¢ test set = 20% hold-out â€¢ random seed 2025)

| Rank | Model                  | ROC-AUC | PR-AUC  | Recall@30% prec. | Precision@30% recall | F1 (churn class) | Inference time (ms) | Training time | Notes / Library version          |
|------|------------------------|---------|---------|------------------|----------------------|------------------|----------------------|---------------|----------------------------------|
| 1    | CatBoost 1.2.8         | **0.903** | **0.701** | 0.792            | 0.314                | **0.612**        | 4.1                  | 4.9 s         | best default tuning + class weights |
| 2    | LightGBM 4.5.0         | 0.899   | 0.689   | 0.781            | 0.309                | 0.601            | **1.8**              | 1.4 s         | fastest good model               |
| 3    | XGBoost 2.1.1          | 0.896   | 0.682   | 0.774            | 0.306                | 0.595            | 3.2                  | 2.8 s         | early stopping helped a lot      |
| 4    | HistGradientBoosting   | 0.887   | 0.665   | 0.752            | 0.298                | 0.576            | 2.9                  | 1.1 s         | scikit-learn native, very stable |
| 5    | RandomForest 1.6.1     | 0.872   | 0.632   | 0.718            | 0.284                | 0.542            | 8.7                  | 4.2 s         | still useful baseline in 2025    |
| 6    | LogisticRegression     | 0.851   | 0.592   | 0.681            | 0.269                | 0.512            | **1.2**              | 0.6 s         | strong linear baseline           |
| 7    | TabPFN (prior 1.0)     | 0.889   | 0.671   | 0.763            | 0.302                | 0.589            | 180â€“450              | ~35 s         | Transformer in-context learning  |
| 8    | AutoGluon 1.2 (best)   | 0.901   | 0.695   | 0.785            | 0.311                | 0.605            | varies               | 180â€“600 s     | ensemble (used as oracle)        |

**Evaluation notes (2025 common practice):**

- Primary business metric â†’ **Recall at fixed 30% precision** (we are willing to contact ~30% of customers if we can catch ~78â€“80% of future churners)
- Secondary metric â†’ **PR-AUC** (better reflects performance on imbalanced classes than ROC-AUC)
- All models trained with **class_weight='balanced'** / **scale_pos_weight** or equivalent
- Hyperparameter tuning: Optuna 3.6+ (60â€“120 trials depending on model)
- Features:  ~18â€“26 after feature selection / engineering
- No leakage, proper temporal split when date column available

**Quick business translation (example â€“ Telco dataset):**

- ~1,400 customers/month at risk  
- Catch 79% â†’ ~1,106 retained (at cost of contacting ~2,100â€“2,300 customers)  
- Assume $15â€“25 retention offer success rate 12â€“18% â†’ realistic monthly saved revenue $2kâ€“$8k depending on ARPU


# 1. Clone repository
git clone https://github.com/yourusername/customer-churn-analysis.git
cd customer-churn-analysis

# 2. Recommended: use uv (fast & modern) or pip
# Option A â€“ uv (2025 favourite)
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt

# Option B â€“ classic
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 3. Run EDA notebook
jupyter lab notebooks/01-eda.ipynb

# 4. Or run full training pipeline
python src/models/train_model.py --model catboost --save
